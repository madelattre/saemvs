---
title: "A quick tutorial on SAEMVS"
author: "Maud Delattre"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 2
    number_sections: true
vignette: >
  %\VignetteIndexEntry{A quick tutorial on SAEMVS}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
library(saemvs)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = TRUE, # montre le code
  results = "markup", # montre les sorties console
  message = TRUE, # garde les messages R
  warning = FALSE, # garde les warnings
  fig.show = "hold", # affiche les graphiques
  fig.align = "center",
  fig.width = 6,
  fig.height = 4
)
```

# Introduction

The **`saemvs`** package is designed for users of nonlinear mixed-effects models who want to perform variable selection, especially when the number of available covariates is very large (high-dimensional variable selection).

`saemvs` implements the first algorithm that performs this task in an automatic way, unlike existing methods that are largely empirical or manual.

The package leverages `C++` code via `Rcpp` to ensure efficient computation even for high-dimensional data.

# Installation

To use the `saemvs` package, first ensure it is installed. Once installed, load the package with:

```{r}
library(saemvs)
```

# Getting started

The package includes example datasets used throughout this vignette to illustrate the main functionalities of `saemvs`.
Two datasets are available:

```{r}
library(saemvs)
data("example_data_list")
data("example_data_df")
```

These two objects contain the same simulated data presented in different formats:

- `example_data_list` provides the data as lists of individual longitudinal observations and a corresponding matrix of candidate covariates. This format is suitable for direct use with `saemvsData()` (see below).
- `example_data_df` stores the same information in data frame form, convenient for use with `saemvsDataFromDFs()` (see below).

The simulated dataset represents a simple growth model, where the response trajectories mimic individual growth curves over time.
Each individual is associated with 200 simulated candidate covariates, among which only a few have an actual effect on the model parameters.
These datasets are designed to provide realistic test cases for illustrating variable selection with `saemvs`. Users interested in the generation procedure can find the script in `inst/scripts`.

## Preparation

First, it is necessary to specify the data structure, the model, and various settings that are useful for running the SAEMVS procedure.

The `saemvsData` object contains the observed responses (`y`), the measurement time points (`t`) and the candidate covariates for variable selection (`x_candidates`). This object serves as the main input for the SAEMVS algorithm and can be created in two different ways, depending on how the data are stored.

When the data are available as lists of longitudinal measurements (one element per individual) and a corresponding matrix of covariates, the `saemvsData()` function can be used directly:

```{r, eval = FALSE}
# Create saemvsData object
saemvs_input <- saemvsData(
  y = example_data_list$y_list,
  t = example_data_list$t_list,
  x_candidates = example_data_list$x
)
```

In this format, each element of `y_list` and `t_list` corresponds to one individual, containing their observed responses and time points respectively.

Alternatively, when the data are stored in data frames — one containing the longitudinal measurements and another containing the covariates — they can be converted automatically using the helper function `saemvsDataFromDFs()`:

```{r}
# Create saemvsData object from dataframes
saemvs_input <- saemvsDataFromDFs(
  long_df = df_long, 
  covar_df = df_cov, 
  id_col = "id", 
  y_col = "y", 
  t_col = "time"
)
```

The `saemvsModel` object specifies the structural model. For this example, we consider a 3-parameter logistic-type model.

```{r}
# Define the model function
g <- function(phi, t) {
  phi[2] + phi[1] / (1 + exp(-(t - phi[3])))
}

# Create saemvsModel object
model <- saemvsModel(
  g = g,
  phi_dim = 3,
  phi_to_select_idx = c(1, 3)
)
```

`g`, the user-defined model function, `phi_dim`, the dimension of the vector of individual parameters and `phi_to_select_idx`, the indices of parameters subject to selection are necessary information assembled in the `saemvsModel` object. Here, we consider that only `phi[1]` and `phi[3]` are subject to selection.

The minimum hyperparameters to be declared in the object are those of the inverse Wishart distribution on the covariance matrix of random effects (*i.e* its scale matrix `cov_re_prior_scale` and degrees of freedom `cov_re_prior_df`.).

```{r}
# Define hyperparameters for the slab
hyper_slab <- saemvsHyperSlab(
  cov_re_prior_scale = diag(rep(0.2, 2)),
  cov_re_prior_df = 4
)
```

The `saemvsTuning` object specifies algorithm tuning parameters, here `niter`: total number of iterations, `nburnin`: number of burn-in iterations and `spike_values_grid`: grid of spike values for variable selection.

```{r}
# Define tuning parameters
tuning <- saemvsTuning(
  niter = 1000,
  nburnin = 800,
  spike_values_grid = exp(-3 + seq(3, 10) * 3 / 4)
)
```

Finally, the `saemvsInit` object specifies initial values for the algorithm.

```{r}
# Define initial values
init_values <- saemvsInit(
  intercept = c(1300, 350, 250),
  beta_candidates = matrix(
    c(
      80, 20, 20, rep(0, 197),
      rep(0, 200),
      15, rep(0, 199)
    ),
    ncol = 3
  ),
  cov_re = diag(c(400, 200, 200)),
  sigma2 = 100
)
```

## Running SAEMVS

Once all objects are prepared (`saemvsData`, `saemvsModel`, `saemvsHyperSlab`, `saemvsTuning`, `saemvsInit`), you can run the main `saemvs` function to perform variable selection.


```{r run_saemvs, eval = FALSE}
# Run the SAEMVS algorithm
result <- saemvs(
  data = saemvs_input,
  model = model,
  init = init_values,
  tuning_algo = tuning,
  hyperparam = hyper_slab,
  pen = "e-BIC"
)
```

## Inspect the results

You can get a summary of the selected variables and parameter estimates in the best model:

```{r load_result, include = FALSE}
result <- readRDS(system.file("extdata", "result_basic_example.rds", package = "saemvs"))
```

```{r saemvs_summary}
summary_saemvs(result)
```

## Diagnostic plots

Visual diagnostics can help evaluate the algorithm's variable selection process.

```{r saemvs_plot}
plots <- prepare_grid_plot(result)
plots$reg_plot[[1]]
plots$reg_plot[[2]]

plots$ebic_plot
```

# Advanced statistical options

## Considering models with fixed-effects

In some situations, one or more parameters are not assumed to vary between individuals, but are instead modeled as fixed effects.
In `saemvs`, this can be specified using the argument `phi_fixed_idx` in the `saemvsModel` constructor.

In the example below, parameter 2 is treated as a fixed effect:

- Fixed effects cannot be subject to variable selection (`phi_fixed_idx` must be disjoint from `phi_to_select_idx`).
- When defining the initial values in `saemvsInit`, the corresponding variance in the random-effects covariance matrix (`cov_re`) must be set to 0. This indicates that this parameter does not vary between individuals.
- The intercept is still initialized in `intercept`.

```{r}
model_with_fixed_effects <- saemvsModel(
  g = g,
  phi_dim = 3,
  phi_to_select_idx = c(1, 3),
  phi_fixed_idx = 2
)

init_values_with_fixed_effects <- saemvsInit(
  intercept = c(1300, 350, 250),
  beta_candidates = matrix(
    c(
      80, 20, 20, rep(0, 197),
      rep(0, 200),
      15, rep(0, 199)
    ),
    ncol = 3
  ),
  cov_re = diag(c(400, 0, 200)),
  sigma2 = 100
)

```

```{r run_saemvs_fixed_effects, eval = FALSE}
# Run the SAEMVS algorithm
result_with_fixed_effects <- saemvs(
  data = saemvs_input,
  model = model_with_fixed_effects,
  init = init_values_with_fixed_effects,
  tuning_algo = tuning,
  hyperparam = hyper_slab,
  pen = "e-BIC"
)
```

```{r load_results_wfe, include = FALSE}
result_with_fixed_effects <- readRDS(system.file("extdata", "result_with_fixed_effects.rds", package = "saemvs"))
```

```{r results_saemvs_fixed_effects}
summary_saemvs(result_with_fixed_effects)
```

## Forcing inclusion of variables

In some applications, it is necessary to force the inclusion of certain covariates in the model, regardless of their statistical support in the selection procedure. For example, one may want to always adjust for a baseline covariate such as treatment group or sex, while allowing other covariates to be selected or discarded.

In `saemvs`, this is done by:

- Passing the covariates to be always included in the argument `x_forced` of `saemvsData`.
- Specifying how these covariates enter each individual parameter through the `x_forced_support` matrix in `saemvsModel`. Each row corresponds to a forced covariate, each column corresponds to a parameter. A value of 1 indicates that the covariate affects the corresponding parameter, 0 otherwise.
- Providing initial values for their coefficients in `beta_forced` within `saemvsInit`.

In the example below, we force the inclusion of the first covariate (column 1 of `x`), and specify that it affects parameters 1 and 3 of the model:

```{r}
# saemvs_input_with_forced_covariates <- saemvsData(
#   y = example_data$y_list,
#   t = example_data$t_list,
#   x_candidates = example_data$x[, -1],
#   x_forced = example_data$x[, 1, drop = FALSE]
# )

saemvs_input_with_forced_covariates <- saemvsDataFromDFs(
  long_df = df_long, 
  covar_df = df_cov, 
  id_col = "id", 
  y_col = "y", 
  t_col = "time",
  x_candidates_cols = seq(2, 200),
  x_forced_cols = c(1)
)

model_with_forced_covariates <- saemvsModel(
  g = g,
  phi_dim = 3,
  phi_to_select_idx = c(1, 3),
  x_forced_support = matrix(c(1, 0, 1), ncol = 3, nrow = 1)
)

init_values_with_forced_covariates <- saemvsInit(
  intercept = c(1300, 350, 250),
  beta_candidates = matrix(
    c(
      20, 20, rep(0, 197),
      rep(0, 199),
      rep(0, 199)
    ),
    ncol = 3
  ),
  beta_forced = matrix(
    c(80, 0, 15),
    ncol = 3, nrow = 1
  ),
  cov_re = diag(c(400, 200, 200)),
  sigma2 = 100
)
```


```{r run_saemvs_forced_covariates, eval= FALSE}
result_with_forced_covariates <- saemvs(
  data = saemvs_input_with_forced_covariates,
  model = model_with_forced_covariates,
  init = init_values_with_forced_covariates,
  tuning_algo = tuning,
  hyperparam = hyper_slab,
  pen = "e-BIC"
)
```

```{r load_results_wfc, include = FALSE}
result_with_forced_covariates <- readRDS(system.file("extdata", "result_with_forced_covariates.rds", package = "saemvs"))
```

```{r result_with_forced_covariates}
summary_saemvs(result_with_forced_covariates, digits = 3)
```

## Initializing coefficients automatically

In some situations, the initialization of the model parameters can be cumbersome, especially when the number of candidate covariates is large or when the user does not have prior knowledge about reasonable starting values.

To simplify this step, `saemvs` provides an automatic initialization procedure through the argument `default = TRUE` in the function `saemvsInit()`.

When `default = TRUE`, the algorithm automatically derives reasonable starting values for the coefficients using a preliminary lasso regression applied to the individual-level latent parameters estimated from separate curve-by-curve fits.
This approach identifies covariates that are likely to be associated with each parameter, and uses the resulting coefficients as initial guesses for the subsequent SAEMVS estimation.

```{r default_init, eval = FALSE}
init_values_default <- saemvsInit(
  intercept = c(1300, 350, 250),
  cov_re = diag(c(400, 0, 200)),
  sigma2 = 100,
  default = TRUE
)
```

When `default = TRUE`, the initialization of the regression coefficients (`beta_candidates` or `beta_forced`) is handled automatically — users should not provide these arguments explicitly.
In practice, the automatic initialization replaces either `beta_candidates` or `beta_forced` (depending on the model specification), but it does not override the other initial components of the model.
The user must still supply initial values for:

- the intercepts (`intercept`),
- the random-effects covariance matrix (`cov_re`),
- and the residual variance (`sigma2`).

This ensures that all model components are properly initialized while allowing the automatic procedure to focus on estimating plausible starting values for the regression coefficients.

## Using BIC instead of e-BIC

By default, `saemvs` uses the extended BIC (e-BIC) as the selection criterion. The e-BIC is particularly suitable for high-dimensional settings, where the number of candidate covariates is large compared to the sample size.

In situations where the covariate set is moderate in size (not high-dimensional), users may prefer to use the classical BIC. This can be done simply by setting `pen = "BIC"` in the `saemvs` function.

It is important to note that:

- The selected model may differ depending on the criterion used (BIC vs e-BIC).
- BIC tends to favor slightly larger models because its penalty for complexity is smaller than that of e-BIC.
- e-BIC includes an additional term to penalize high-dimensional covariate spaces, which helps avoid overfitting when p is large.

In the example below, we run the SAEMVS algorithm using the BIC criterion instead of e-BIC.

```{r run_saemvs_bic, eval = FALSE}
# Run the SAEMVS algorithm
result_with_bic <- saemvs(
  data = saemvs_input,
  model = model,
  init = init_values,
  tuning_algo = tuning,
  hyperparam = hyper_slab,
  pen = "BIC"
)
```


# Technical options for computation

## Calibrating tuning parameters

The performance of the SAEMVS algorithm can depend on the choice of tuning parameters, in particular the grid of spike values controlling the sparsity level in the variable selection step.
To assist users in exploring and calibrating these parameters, `saemvs` provides a convenient workflow that allows running the algorithm for a single value of the spike grid and inspecting the convergence behavior of the SAEM algorithm.

This can be achieved by using the `test_saemvs` function which performs maximum a posteriori parameter estimation for one single spike value in the tuning grid.
The resulting object can then be visualized using the function `convergence_plot()`, which helps assess whether the estimation algorithm has stabilized.

```{r test, eval = FALSE}
res_test <- test_saemvs(
  data = saemvs_input, 
  model = model, 
  init = init_values, 
  tuning_algo = tuning,
  hyperparam = hyper_slab
  )
```

```{r load_res_test, include = FALSE}
res_test <- readRDS(system.file("extdata", "res_test.rds", package = "saemvs"))
```

```{r conv_plt}
# Visualizing convergence for the most variable coefficients
convergence_plot(res_test, "coef_phi_sel", "top:16", phi = 1)
```

In this example, the plot displays the trajectories of the 16 candidate covariate coefficients (`"coef_phi_sel"`) on first model parameter (`phi = 1`) - including the intercept - that exhibit the largest fluctuations over the iterations.
Such diagnostic plots are useful to check that the algorithm has reached convergence and identify potential instabilities or poor initializations.

Once satisfactory convergence is observed for a given value of the spike, the user can proceed to run the full grid to perform model selection using the chosen penalization criterion (e.g. e-BIC or BIC) with the `saemvs` function.

## Managing computational resources

In addition to statistical settings, `saemvs` provides technical options to optimize computation time and efficiency. The main way to speed up computation is to run the algorithm in parallel across multiple cores. This is particularly useful when the number of candidate covariates is large and the grid of spike values is wide. By default, `saemvs` uses 4 workers (`nb_workers = 4`). Users can increase this number to take advantage of additional CPU cores. The number of cores used is controlled by the `nb_workers` argument in the `saemvsTuning` object.

In the example below, we use 8 workers to parallelize the algorithm:

```{r}
tuning_workers <- saemvsTuning(
  niter = 1000,
  nburnin = 800,
  spike_values_grid = exp(-3 + seq(1, 8)),
  nb_workers = 8
)
```

Running SAEMVS with this tuning object will distribute computation across the specified number of workers, significantly reducing runtime in high-dimensional or computationally intensive settings.

<!-- Note: Parallelisation gains are most noticeable when the number of covariates or spike values is large. For small datasets, using multiple workers may not provide a speed-up and can even add overhead. -->
